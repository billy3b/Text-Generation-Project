{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0893aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas\n",
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af00df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('frankenstein.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27c58631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    input = input.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "    \n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5a9af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c,i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "957617b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of characters = 269079\n",
      "total vocab= 38\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print(\"total number of characters =\", input_len)\n",
    "print(\"total vocab=\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fff983b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8320c327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total patterns= 268979\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, input_len - seq_len,1):\n",
    "    in_seq = processed_inputs[i:i + seq_len]\n",
    "    out_seq = processed_inputs[i + seq_len]\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append([char_to_num[out_seq]])\n",
    "    \n",
    "n_patterns = len(x_data)\n",
    "print('total patterns=', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a66a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_len, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0edaf4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4460f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences= True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6d70dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cce0116",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'model_weights_saved_new.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss', verbose = 1 , save_best_only= True, mode ='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03491a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1051/1051 [==============================] - ETA: 0s - loss: 2.8963\n",
      "Epoch 1: loss improved from inf to 2.89628, saving model to model_weights_saved_new.hdf5\n",
      "1051/1051 [==============================] - 5589s 5s/step - loss: 2.8963\n",
      "Epoch 2/4\n",
      "1051/1051 [==============================] - ETA: 0s - loss: 2.6182\n",
      "Epoch 2: loss improved from 2.89628 to 2.61820, saving model to model_weights_saved_new.hdf5\n",
      "1051/1051 [==============================] - 6419s 6s/step - loss: 2.6182\n",
      "Epoch 3/4\n",
      "1051/1051 [==============================] - ETA: 0s - loss: 2.4568 \n",
      "Epoch 3: loss improved from 2.61820 to 2.45684, saving model to model_weights_saved_new.hdf5\n",
      "1051/1051 [==============================] - 11271s 11s/step - loss: 2.4568\n",
      "Epoch 4/4\n",
      "1051/1051 [==============================] - ETA: 0s - loss: 2.3444\n",
      "Epoch 4: loss improved from 2.45684 to 2.34437, saving model to model_weights_saved_new.hdf5\n",
      "1051/1051 [==============================] - 8420s 8s/step - loss: 2.3444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bf4e2f68b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y, epochs=4, batch_size = 256 , callbacks= desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9be401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_weights_saved_new.hdf5'\n",
    "model.load_weights(filename)\n",
    "model.compile(loss ='categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7579e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i,c) for i,c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c34b5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ramdim seed =\n",
      "\" serable apparatus dungeon morning remember thus awoke understanding forgotten particulars happened f \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print('Ramdim seed =')\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed6564ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arher seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare seare sear"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1,len(pattern), 1))\n",
    "    x = x/float(vocab_len)\n",
    "    prediction = model.predict(x, verbose = 0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d181164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
